from fastapi import FastAPI, Query, HTTPException, UploadFile, File
from fastapi.responses import StreamingResponse
from faster_whisper import WhisperModel
import time
import os
import json
import asyncio
from typing import Optional, AsyncGenerator
import tempfile
import uvicorn
from pathlib import Path


app = FastAPI(title="è¯­éŸ³è½¬å½•æœåŠ¡", description="åŸºäº faster-whisper çš„å®æ—¶è¯­éŸ³è½¬å½• API")

# å…¨å±€æ¨¡å‹ç¼“å­˜
_model_cache = {}


def get_file_type(file_path):
    """è·å–æ–‡ä»¶ç±»å‹"""
    file_ext = os.path.splitext(file_path)[1].lower()
    
    # éŸ³é¢‘æ ¼å¼
    audio_formats = {'.mp3', '.wav', '.m4a', '.aac', '.ogg', '.flac', '.wma'}
    # è§†é¢‘æ ¼å¼
    video_formats = {'.mp4', '.avi', '.mov', '.mkv', '.wmv', '.flv', '.webm', '.m4v', '.3gp'}
    
    if file_ext in audio_formats:
        return 'audio'
    elif file_ext in video_formats:
        return 'video'
    else:
        return 'unknown'


def get_model(model_size="medium", device="cuda", compute_type="float16"):
    """è·å–æˆ–åˆ›å»ºæ¨¡å‹å®ä¾‹ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
    cache_key = f"{model_size}_{device}_{compute_type}"
    
    if cache_key not in _model_cache:
        print(f"ğŸ”§ æ­£åœ¨åŠ è½½ {model_size} æ¨¡å‹...")
        _model_cache[cache_key] = WhisperModel(
            model_size, device=device, compute_type=compute_type
        )
        print(f"âœ… æ¨¡å‹ {model_size} åŠ è½½å®Œæˆ")
    
    return _model_cache[cache_key]


async def transcribe_media_stream(
    media_path: str,
    model_size: str = "medium",
    device: str = "cuda", 
    compute_type: str = "float16",
    beam_size: int = 5,
    language: str = "zh"
) -> AsyncGenerator[str, None]:
    """æµå¼è½¬å½•åª’ä½“æ–‡ä»¶ï¼Œå®æ—¶è¿”å›ç»“æœ"""
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(media_path):
        raise FileNotFoundError(f"åª’ä½“æ–‡ä»¶ä¸å­˜åœ¨: {media_path}")
    
    # æ£€æµ‹æ–‡ä»¶ç±»å‹
    file_type = get_file_type(media_path)
    
    # å‘é€å¼€å§‹ä¿¡æ¯
    start_info = {
        "type": "start",
        "file_name": os.path.basename(media_path),
        "file_type": file_type,
        "timestamp": time.time()
    }
    yield f"data: {json.dumps(start_info, ensure_ascii=False)}\n\n"
    
    try:
        # è·å–æ¨¡å‹
        model = get_model(model_size, device, compute_type)
        
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()
        
        # è½¬å½•åª’ä½“æ–‡ä»¶
        segments, info = model.transcribe(media_path, beam_size=beam_size, language=language)
        
        # å‘é€è¯­è¨€æ£€æµ‹ä¿¡æ¯
        lang_info = {
            "type": "language_detected",
            "language": info.language,
            "language_probability": float(info.language_probability),
            "timestamp": time.time()
        }
        yield f"data: {json.dumps(lang_info, ensure_ascii=False)}\n\n"
        
        # å®æ—¶å¤„ç†æ¯ä¸ªéŸ³é¢‘æ®µ
        segment_count = 0
        
        for segment in segments:
            segment_count += 1
            
            # å‘é€è½¬å½•æ®µç»“æœ
            segment_data = {
                "type": "segment",
                "segment_id": segment_count,
                "start_time": float(segment.start),
                "end_time": float(segment.end),
                "text": segment.text,
                "timestamp": time.time()
            }
            yield f"data: {json.dumps(segment_data, ensure_ascii=False)}\n\n"
            
            # è®©å‡ºæ§åˆ¶æƒï¼Œå…è®¸å…¶ä»–ä»»åŠ¡æ‰§è¡Œ
            await asyncio.sleep(0.01)
        
        # è®¡ç®—æ€»è€—æ—¶
        end_time = time.time()
        elapsed_time = end_time - start_time
        
        # å‘é€å®Œæˆä¿¡æ¯
        complete_info = {
            "type": "complete",
            "total_segments": segment_count,
            "elapsed_time": elapsed_time,
            "file_type": file_type,
            "timestamp": time.time()
        }
        yield f"data: {json.dumps(complete_info, ensure_ascii=False)}\n\n"
        
    except Exception as e:
        # å‘é€é”™è¯¯ä¿¡æ¯
        error_info = {
            "type": "error",
            "error_message": str(e),
            "timestamp": time.time()
        }
        yield f"data: {json.dumps(error_info, ensure_ascii=False)}\n\n"


@app.get("/")
async def root():
    """API æ ¹ç›®å½•"""
    return {"message": "ğŸµ è¯­éŸ³è½¬å½•æœåŠ¡è¿è¡Œä¸­", "version": "1.0.0"}


@app.get("/asr")
async def transcribe_by_path(
    media_path: str = Query(..., description="åª’ä½“æ–‡ä»¶çš„å®Œæ•´è·¯å¾„"),
    model_size: str = Query("medium", description="æ¨¡å‹å¤§å° (small/medium/large-v3)"),
    device: str = Query("cuda", description="è®¾å¤‡ç±»å‹ (cuda/cpu)"),
    compute_type: str = Query("float16", description="è®¡ç®—ç±»å‹ (float16/int8_float16/int8)"),
    beam_size: int = Query(5, description="é›†æŸæœç´¢å¤§å°"),
    language: str = Query("zh", description="è¯­è¨€ä»£ç ")
):
    """é€šè¿‡æ–‡ä»¶è·¯å¾„è¿›è¡Œè¯­éŸ³è½¬å½• (Server-Sent Events)"""
    
    return StreamingResponse(
        transcribe_media_stream(
            media_path=media_path,
            model_size=model_size,
            device=device,
            compute_type=compute_type,
            beam_size=beam_size,
            language=language
        ),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Headers": "*",
        }
    )

# ã€é¢„ç•™æ–¹æ¡ˆã€‘ç›´æ¥ä¸Šä¼ æ–‡ä»¶åˆ°æœåŠ¡ç«¯ï¼Œè¿›è¡Œè½¬å½•
@app.post("/asr/upload")
async def transcribe_by_upload(
    file: UploadFile = File(...),
    model_size: str = Query("medium", description="æ¨¡å‹å¤§å°"),
    device: str = Query("cuda", description="è®¾å¤‡ç±»å‹"),
    compute_type: str = Query("float16", description="è®¡ç®—ç±»å‹"),
    beam_size: int = Query(5, description="é›†æŸæœç´¢å¤§å°"),
    language: str = Query("zh", description="è¯­è¨€ä»£ç ")
):
    """é€šè¿‡æ–‡ä»¶ä¸Šä¼ è¿›è¡Œè¯­éŸ³è½¬å½• (Server-Sent Events)"""
    
    # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•
    with tempfile.NamedTemporaryFile(delete=False, suffix=Path(file.filename).suffix) as tmp_file:
        content = await file.read()
        tmp_file.write(content)
        temp_path = tmp_file.name
    
    try:
        return StreamingResponse(
            transcribe_media_stream(
                media_path=temp_path,
                model_size=model_size,
                device=device,
                compute_type=compute_type,
                beam_size=beam_size,
                language=language
            ),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "Access-Control-Allow-Origin": "*",
                "Access-Control-Allow-Headers": "*",
            }
        )
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        try:
            os.unlink(temp_path)
        except:
            pass


if __name__ == "__main__":
    print("ğŸš€ å¯åŠ¨è¯­éŸ³è½¬å½•æœåŠ¡...")
    uvicorn.run(
        app, 
        host="0.0.0.0", 
        port=8000,
        log_level="info"
    )